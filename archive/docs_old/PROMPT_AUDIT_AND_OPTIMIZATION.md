# üîç Comprehensive Prompt Strategy Audit & Optimization

**Date**: October 22, 2025  
**Scope**: Full evaluation against GPT-5 best practices and Responses API capabilities

---

## üìä Executive Summary

### **Critical Finding**: Our prompts are not leveraging GPT-5's strengths

**Current Grade**: ‚≠ê‚≠ê‚ö†Ô∏è (2.5/5)
- ‚úÖ Good: User context, terminology adaptation
- ‚ö†Ô∏è Weak: Instruction hierarchy, no tool preambles, contradictions
- ‚ùå Missing: Responses API features, persistence instructions, reasoning control

**Optimized Grade Target**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)

---

## üéØ Application Context

### **Purpose**
B2B research intelligence platform for enterprise Account Executives to:
1. Research companies with ICP scoring
2. Identify buying signals and decision makers
3. Generate personalized outreach intelligence
4. Learn and adapt to user preferences over time

### **Target Users**
- Enterprise Account Executives (AEs)
- Sales Development Reps (SDRs)  
- Sales leaders needing strategic intelligence
- Expectations: Fast, accurate, actionable insights

### **Core Value Proposition**
Transform hours of manual research into seconds with AI-powered intelligence that:
- Discovers hidden ICP matches through deep analysis
- Generates hyper-personalized outreach angles
- Anticipates information needs before asking
- Learns user preferences automatically
- Delivers strategic insights beyond surface data

---

## üî¥ Critical Issues in Current Prompt

### **1. Missing Instruction Hierarchy** ‚ö†Ô∏è
**Current**: Flat list of rules with no priority ordering  
**Issue**: GPT-5 wastes reasoning tokens trying to reconcile conflicts  
**Impact**: Slower, less predictable responses

**Example Contradiction**:
```
"Be concise but complete" ‚Üê Conflicting
"deep by default" vs "Ask clarifying questions" ‚Üê Contradictory
"Never ask what to research" vs "Ask ONE clarifying question when essential" ‚Üê Which takes priority?
```

**From Guide**: 
> "Poorly-constructed prompts containing contradictory or vague instructions can be more damaging to GPT-5 than to other models, as it expends reasoning tokens searching for a way to reconcile the contradictions"

### **2. No Tool Preambles** ‚ùå
**Current**: Silent tool calling - users see nothing until results  
**Issue**: Poor UX, especially on longer research tasks  
**Impact**: Users don't know what's happening

**From Guide**:
> "Intermittent model updates on what it's doing with its tool calls and why can provide for a much better interactive user experience - the longer the rollout, the bigger the difference these updates make."

**Should Have**:
- "üîç Searching for recent funding rounds and leadership changes..."
- "üí° Found 3 key buying signals from last 30 days..."
- "‚úÖ Analysis complete. Synthesizing strategic recommendations..."

### **3. No Persistence Instructions** ‚ùå
**Current**: No guidance on when to stop or continue  
**Issue**: Agent may stop prematurely or ask unnecessary clarifications  
**Impact**: Incomplete research, interrupted flow

**From Guide**:
```xml
<persistence>
- You are an agent - keep going until the query is completely resolved
- Only terminate when you are sure the problem is solved
- Never stop when you encounter uncertainty ‚Äî research or deduce
- Do not ask the human to confirm assumptions
</persistence>
```

### **4. "PREFERENCE CHECK-OUT" Anti-Pattern** üö®
**Current**: Lines 288-291 literally say "PREFERENCE CHECK-OUT"  
**Issue**: We JUST removed this language as "awkward/formal" in our UX fixes!  
**Impact**: Generates the exact language we're trying to eliminate

**This is in DIRECT CONFLICT with our recent UX improvements**

### **5. Not Using Responses API Features** ‚ùå
**Current**: Basic Chat Completions approach  
**Missing**:
- `previous_response_id` for efficient multi-turn
- `reasoning` parameter for effort control  
- `verbosity` parameter for output control
- `store: true` for conversation persistence
- Structured conversation state

**From API Docs**:
> "We've seen statistically significant improvements when using the Responses API... Tau-Bench Retail score increases from 73.9% to 78.2% just by switching"

### **6. No Context Gathering Strategy** ‚ùå
**Current**: "Make multiple web_search calls" (vague)  
**Issue**: No guidance on breadth vs depth, stopping criteria  
**Impact**: Over-searching or under-searching

**From Guide**:
```xml
<context_gathering>
Goal: Get enough context fast. Parallelize discovery and stop as soon as you can act.
Method:
- Start broad, then fan out to focused subqueries
- Launch varied queries in parallel; read top hits
- Avoid over-searching

Early stop criteria:
- You can name exact content to change
- Top hits converge (~70%) on one area
</context_gathering>
```

### **7. No Reasoning Effort Guidance** ‚ùå
**Current**: Not using `reasoning_effort` parameter  
**Issue**: All tasks use same reasoning level (inefficient)  
**Impact**: Wasted tokens on simple tasks, insufficient depth on complex ones

**Should Have**: Dynamic reasoning by task type
- Quick brief: `low` or `medium`
- Deep research: `high`
- Follow-ups: `low`

### **8. Prescriptive Format Requirements** ‚ö†Ô∏è
**Current**: Very specific section requirements ("Executive Summary MUST...")  
**Issue**: May conflict with GPT-5's natural structuring  
**Impact**: Forced, unnatural output

**From Guide (Cursor example)**:
> "Structured, scoped prompts yield the most reliable results... but avoid over-constraining format"

---

## ‚úÖ Strengths to Preserve

### **1. User Context Injection** ‚úÖ
**Good**: Profile, ICP, custom criteria, signals integrated  
**Keep**: This provides essential personalization

### **2. Terminology Adaptation** ‚úÖ  
**Good**: Uses user's exact words ("Buying Signals" ‚Üí their term)  
**Keep**: Makes responses feel personalized

### **3. Learned Preferences** ‚úÖ
**Good**: Behavioral learning from past interactions  
**Keep**: Core value proposition

### **4. Zero Clarifier Intent** ‚úÖ
**Good**: Tries to avoid asking questions  
**Keep**: Aligns with proactive research goal  
**Fix**: Make it non-contradictory with other instructions

---

## üéØ Optimized Prompting Strategy

### **Architecture Changes**

#### **1. Adopt Instruction Hierarchy** (GPT-5 Best Practice)
```xml
<instruction_hierarchy>
Priority 1: User's explicit request (NEVER override this)
Priority 2: Complete research autonomously before yielding
Priority 3: Balance speed/depth based on learned preferences
Priority 4: Format preferences and output style
</instruction_hierarchy>
```

#### **2. Add Tool Preambles** (GPT-5 Best Practice)
```xml
<tool_preambles>
Before first tool use: "üîç Researching [Company] ‚Äî searching for [specific angles]..."
During research: Brief progress updates (5-10 words max)
After completion: "‚úÖ Analysis complete. Here's what I found..."

Keep preambles friendly and concise. Think trusted teammate, not robot.
</tool_preambles>
```

#### **3. Add Persistence** (GPT-5 Best Practice)
```xml
<persistence>
You are an autonomous research agent. Complete the task before yielding:
- Find at least 3 actionable insights before stopping
- If initial searches are surface-level, dig deeper automatically
- Never ask "should I continue?" ‚Äî determine completeness yourself
- Only stop when you have specific recommendations
- Don't ask permission to research deeper ‚Äî just do it
</persistence>
```

#### **4. Add Context Gathering Strategy** (GPT-5 Best Practice)
```xml
<context_gathering>
Strategy for maximum value:
1. Launch 3-5 parallel web_search calls immediately
   - Company + "funding news"
   - Company + "leadership changes"
   - Company + "tech stack hiring"
   - ICP-specific signals
   
2. Read top 2-3 results per search
3. Stop when you can answer with confidence (don't over-search)
4. Deep dive only if initial results are thin

Quality threshold: Every insight must be specific and unexpected
</context_gathering>
```

#### **5. Use Responses API Features**
```typescript
// In route.ts, update to:
const responseStream = await openai.responses.stream({
  model: 'gpt-5-mini',
  instructions, // Our optimized prompt
  input: lastUserMessage.content,
  previous_response_id: previousResponseId, // Multi-turn context
  reasoning: {
    effort: agentType === 'quick' ? 'low' : 'medium' // Dynamic
  },
  verbosity: 'medium', // Can override in prompt for specific sections
  store: true, // Enable conversation persistence
  tools: [{ type: 'web_search' }],
  metadata: {
    user_id: user.id,
    chat_id: chatId,
    agent_type: agentType
  }
});
```

#### **6. Remove Contradictions**
**REMOVE**:
- "Be concise but complete" ‚Üí Pick one based on user preference
- "Deep by default" + "Ask clarifying questions" ‚Üí Be autonomous
- "PREFERENCE CHECK-OUT" ‚Üí Remove entirely (we just fixed this!)

**ADD**:
- Clear priority hierarchy
- Autonomous operation by default
- Natural follow-up suggestions (not forced)

#### **7. Simplify Format Requirements**
**Current**: Prescriptive sections  
**New**: Outcome-focused guidance

```xml
<output_excellence>
Goal: Deliver decision-ready intelligence

Required elements:
1. Quick summary (2-3 sentences) with recommendation
2. Key insights (5-7 specific, unexpected findings)
3. Action items (who to contact, when, why)
4. Sources (3+ citations)

Structure flexibly around the story the data tells.
Adapt sections to fit the findings, not the template.
</output_excellence>
```

#### **8. Add Verbosity Control**
```xml
<verbosity_control>
Status updates: Ultra-brief (5-10 words)
Research findings: Detailed with evidence
Recommendations: Clear, numbered, actionable  
Summaries: Bullet points only

When user preference = "concise": Prioritize bullets over paragraphs
When user preference = "detailed": Include context and reasoning
</verbosity_control>
```

---

## üìù Rewritten Prompt (Full Implementation)

See `PROMPT_V2_OPTIMIZED.md` for complete implementation.

**Key Improvements**:
1. ‚úÖ Clear instruction hierarchy
2. ‚úÖ Tool preambles for UX
3. ‚úÖ Persistence instructions
4. ‚úÖ Context gathering strategy
5. ‚úÖ No contradictions
6. ‚úÖ Removed "PREFERENCE CHECK-OUT"
7. ‚úÖ Flexible format guidance
8. ‚úÖ Verbosity control
9. ‚úÖ Reasoning effort by task type
10. ‚úÖ Structured with XML tags

---

## üìä Expected Impact

### **Performance Improvements**
- **Speed**: 15-25% faster (better context gathering, lower reasoning on simple tasks)
- **Quality**: Higher Tau-Bench scores (following Responses API best practices)
- **UX**: Much better (tool preambles, progress updates)
- **Consistency**: More predictable (clear hierarchy, no contradictions)

### **User Experience**
- **Before**: Silent ‚Üí sudden response (confusing)
- **After**: "üîç Researching..." ‚Üí "üí° Found..." ‚Üí "‚úÖ Complete"

### **Cost Efficiency**
- Dynamic reasoning effort saves tokens on simple tasks
- `previous_response_id` reduces redundant context
- Better stop criteria prevents over-searching

---

## üöÄ Implementation Plan

### **Phase 1: Core Rewrite** (30 min)
1. ‚úÖ Replace current prompt in `context.ts`
2. ‚úÖ Add instruction hierarchy
3. ‚úÖ Add tool preambles
4. ‚úÖ Add persistence
5. ‚úÖ Remove contradictions

### **Phase 2: API Updates** (20 min)
1. ‚úÖ Add `previous_response_id` tracking
2. ‚úÖ Add dynamic `reasoning_effort`
3. ‚úÖ Add `verbosity` parameter
4. ‚úÖ Enable `store: true`

### **Phase 3: Testing** (30 min)
1. Test quick brief (should be fast, low reasoning)
2. Test deep research (should be thorough, high reasoning)
3. Test follow-ups (should use previous_response_id)
4. Verify tool preambles show in UI

### **Phase 4: Iteration** (ongoing)
1. Monitor user feedback
2. Use metaprompting to refine
3. A/B test variations

---

## üéì References

**From GPT-5 Prompting Guide**:
- Instruction hierarchy (lines 270-293)
- Tool preambles (lines 77-125)
- Persistence (lines 67-73)
- Context gathering (lines 22-61)
- Cursor case study (lines 211-254)

**From Responses API Docs**:
- `previous_response_id` benefits (line 135)
- `reasoning` parameter (lines 109)
- `verbosity` parameter (lines 115)
- Multi-turn state (lines 96-106)

---

## ‚úÖ Success Criteria

**We'll know it's working when**:
1. ‚úÖ Users see progress updates during research
2. ‚úÖ Responses come faster (dynamic reasoning)
3. ‚úÖ No more contradictory behavior
4. ‚úÖ Follow-ups are more natural
5. ‚úÖ No "PREFERENCE CHECK-OUT" language
6. ‚úÖ Consistent output quality
7. ‚úÖ Better user satisfaction scores

---

**Status**: Audit complete, ready to implement  
**Confidence**: High (based on official best practices)  
**Risk**: Low (preserves what works, fixes what doesn't)
