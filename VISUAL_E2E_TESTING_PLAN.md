# üéØ Visual E2E Testing Plan - Real User Simulation

**Based on**: Real User Testing Blueprint (No Playwright automation)  
**Approach**: Manual browser testing simulating actual user behavior  
**Goal**: Validate 10/10 UAT compliance through realistic user flows

---

## 0Ô∏è‚É£ **What "10/10" Means (First Principles)**

### **Outcome Over Pixels**
- ‚úÖ Users accomplish core jobs-to-be-done quickly and confidently
- ‚úÖ AE's meeting prep, deep company intel, account monitoring work seamlessly
- ‚úÖ No ambiguity or dead-ends in any flow

### **End-to-End Integrity**
- ‚úÖ Signup ‚Üí Approval ‚Üí Onboarding ‚Üí Dashboard ‚Üí Research ‚Üí Save ‚Üí Revisit
- ‚úÖ All transitions smooth and logical
- ‚úÖ No broken states or confusion points

### **Agent Quality**
- ‚úÖ Research outputs accurate, actionable, source-backed
- ‚úÖ Formatted as promised by agent specs
- ‚úÖ Quick vs Deep distinction clear

### **Accessibility & Usability**
- ‚úÖ SUS ‚â• 85 (System Usability Scale)
- ‚úÖ SEQ ‚â• 6/7 (Single Ease Question)
- ‚úÖ Zero critical blockers in P0 flows

---

## 1Ô∏è‚É£ **Test Environments & Setup**

### **Test Accounts**
1. **New User** (no data)
   - Fresh signup
   - No chat history
   - Full credits

2. **Returning User** (seeded data)
   - 3 saved companies (Stripe, Notion, Salesforce)
   - Chat history with 5+ conversations
   - Low credit balance (8 credits)

### **Sample Data**
- **CSV Files**:
  - `valid_10_companies.csv` (10 rows, clean)
  - `valid_50_companies.csv` (50 rows, clean)
  - `malformed_companies.csv` (missing columns, bad data)

### **Devices/Browsers**
- Desktop Chrome (primary)
- Desktop Safari
- iOS Safari (mobile)
- Android Chrome (mobile)

### **Network Profiles**
- Normal (fast connection)
- Slow 3G (to test streaming/latency)

---

## 2Ô∏è‚É£ **The 10 Core Tasks**

### **T1: Sign Up and Get In**
**Goal**: "Create an account and get to the main dashboard."

**Success Criteria**:
- [ ] Completes signup form without errors
- [ ] Receives approval email (or pending approval message)
- [ ] Can re-enter after approval
- [ ] Reaches dashboard without help
- [ ] No dead-ends or confusion

**Time Expectation**: ‚â§ 2 minutes  
**SEQ Target**: ‚â• 6/7

**What to Observe**:
- Email approval handoff clarity
- "Pending approval" messaging
- Re-entry flow after approval

---

### **T2: The 5-Step Onboarding**
**Goal**: "Set up your company so the agent can research for you."

**Success Criteria**:
- [ ] Completes in ‚â§ 3 minutes (README claim)
- [ ] Company name vs URL distinction clear
- [ ] Progress visibility throughout
- [ ] No redundant questions
- [ ] No unnecessary steps
- [ ] Form copy disambiguates inputs

**Time Expectation**: ‚â§ 3 minutes  
**SEQ Target**: ‚â• 6/7

**What to Observe**:
- Clarity of each step
- Progress indicators
- Skip vs required fields
- Completion confirmation

---

### **T3: Quick Brief on a Company**
**Goal**: "Get a quick brief on Salesforce you could skim before a call."

**Success Criteria**:
- [ ] Output matches quick brief schema
- [ ] Includes executive summary
- [ ] Shows key decision makers
- [ ] Lists buying signals
- [ ] Cites sources
- [ ] Feels immediately usable for meeting prep

**Time Expectation**: ‚â§ 2 minutes  
**SEQ Target**: ‚â• 6/7

**Output Quality Rating** (1-5 each):
- [ ] Actionability (explicit next steps)
- [ ] Signal coverage (buying/timing signals)
- [ ] Decision-maker clarity
- [ ] Personalization hooks
- [ ] Evidence (source grounding)

**Target**: Average ‚â• 4/5

---

### **T4: Deep Intelligence Run**
**Goal**: "Now get a deep intelligence report on Salesforce for outreach angles."

**Success Criteria**:
- [ ] Delivers full structure (summary ‚Üí strategic intelligence ‚Üí recommendations ‚Üí personalization)
- [ ] Noticeably richer than quick brief
- [ ] User can articulate next action
- [ ] Sources cited throughout
- [ ] Formatted professionally

**Time Expectation**: ‚â§ 10 minutes  
**SEQ Target**: ‚â• 6/7

**Output Quality Rating** (1-5 each):
- [ ] Actionability
- [ ] Signal coverage
- [ ] Decision-maker clarity
- [ ] Personalization hooks
- [ ] Evidence

**Target**: Average ‚â• 4/5 (should exceed Quick Brief)

---

### **T5: Save & Find It Later**
**Goal**: "Save this research and find it again."

**Success Criteria**:
- [ ] Can save research without confusion
- [ ] Saved item appears in correct location
- [ ] Can find saved research in ‚â§ 30 seconds
- [ ] Naming/metadata clear
- [ ] Persistence to research_outputs verified

**Time Expectation**: ‚â§ 1 minute (save + find)  
**SEQ Target**: ‚â• 6/7

**What to Observe**:
- Save button visibility
- Confirmation feedback
- Discoverability from dashboard
- Search/filter functionality

---

### **T6: Configure via Settings Agent**
**Goal**: "Tell the system the industries, company sizes, and buying signals you care about going forward."

**Success Criteria**:
- [ ] Configuration via natural language works
- [ ] Agent proposes structured changes
- [ ] Offers to save globally
- [ ] Settings update reflected in future research
- [ ] Feels faster than forms

**Time Expectation**: ‚â§ 3 minutes  
**SEQ Target**: ‚â• 6/7

**What to Observe**:
- Conversational flow quality
- Agent understanding of requests
- Confirmation of changes
- Settings persistence

---

### **T7: Bulk Research from CSV**
**Goal**: "Upload a CSV of companies and get results you can download."

**Success Criteria**:
- [ ] File validation works (accepts valid, rejects invalid)
- [ ] Progress status visible
- [ ] Job survives page refresh
- [ ] Can download results
- [ ] CSV output has expected columns
- [ ] Quality matches individual research

**Time Expectation**: ‚â§ 5 minutes (upload + monitor)  
**SEQ Target**: ‚â• 6/7

**What to Observe**:
- Upload UX
- Progress indicators
- Error handling (malformed CSV)
- Download process
- Output quality

---

### **T8: Credits & Limits**
**Goal**: "Kick off research until you hit a low balance‚Äîwhat happens?"

**Success Criteria**:
- [ ] Clear feedback/warnings before hitting limit
- [ ] No silent failures
- [ ] User knows how to purchase or manage usage
- [ ] Credit deductions visible
- [ ] Low balance warning appears

**Time Expectation**: ‚â§ 2 minutes  
**SEQ Target**: ‚â• 6/7

**What to Observe**:
- Warning thresholds
- Messaging clarity
- Purchase flow (if applicable)
- Balance visibility

---

### **T9: Come Back Tomorrow (Re-entry)**
**Goal**: "Return later and pick up where you left off."

**Success Criteria**:
- [ ] Fast recall of context
- [ ] Saved threads obvious
- [ ] No cognitive re-onboarding needed
- [ ] Recent activity visible
- [ ] Can resume conversations

**Time Expectation**: ‚â§ 1 minute  
**SEQ Target**: ‚â• 6/7

**What to Observe**:
- Dashboard state on return
- Chat history accessibility
- Context preservation
- Navigation clarity

---

### **T10: Accessibility Spot Checks**
**Goal**: "Complete a quick brief and save it‚Äîkeyboard only / screen reader."

**Success Criteria** (AA Compliance):
- [ ] Logical focus order
- [ ] Visible focus outlines
- [ ] All controls labeled
- [ ] No keyboard traps
- [ ] Screen reader announces correctly
- [ ] Contrast ratios sufficient
- [ ] Target sizes adequate

**Time Expectation**: Same as T3 + T5  
**SEQ Target**: ‚â• 6/7

**What to Test**:
- Tab navigation through forms
- Enter/Space activation
- Escape to close dialogs
- Arrow keys in lists
- Screen reader labels

---

## 3Ô∏è‚É£ **Pass/Fail Rubrics**

### **Flow Completion & Efficiency**

**üü¢ Pass (Green)**:
- Unassisted completion
- ‚â§ Expected time
- ‚â§ Expected clicks
- No backtracks

**üü° Yellow**:
- Minor detours
- Brief assistance needed
- Slightly over time

**üî¥ Red (Fail)**:
- Cannot complete
- Requires heavy facilitation
- Major confusion points

### **Research Output Quality** (1-5 scale)

**Criteria**:
1. **Actionability**: Explicit next steps provided
2. **Signal Coverage**: Buying/timing signals identified
3. **Decision-Maker Clarity**: Key contacts named with context
4. **Personalization Hooks**: Specific angles for outreach
5. **Evidence**: Sources cited and credible

**Target**: Average ‚â• 4/5 for all outputs

**Deep vs Quick**:
- Deep should score ‚â• 0.5 points higher on all criteria
- If not, that's a red flag

### **Accessibility (AA)**

**Must Pass**:
- Keyboard reachability (all interactive elements)
- Visible focus indicators
- Proper labeling (aria-labels, form labels)
- Contrast ratios ‚â• 4.5:1
- Error messaging clarity
- No keyboard traps

---

## 4Ô∏è‚É£ **Success Thresholds (Ship/No-Ship Gates)**

### **P0 Tasks (T1-T7)**
- ‚úÖ ‚â• 95% unassisted completion
- ‚úÖ Median times:
  - T2 ‚â§ 3 min (onboarding)
  - T3 ‚â§ 2 min (quick brief)
  - T4 ‚â§ 10 min (deep intelligence)
  - T7 ‚â§ 5 min (bulk CSV)

### **Usability Scores**
- ‚úÖ SUS ‚â• 85
- ‚úÖ SEQ median ‚â• 6/7 on P0 tasks

### **Quality**
- ‚úÖ Output quality average ‚â• 4/5 on all five criteria
- ‚úÖ Deep outputs noticeably exceed Quick outputs

### **Blockers**
- ‚úÖ Zero critical blockers
- ‚úÖ AA compliance on tested flows

---

## 5Ô∏è‚É£ **Testing Checklist (Per Task)**

### **Observer Sheet Template**

```
Task ID: T___
Participant: ___________
Start Time: __:__
End Time: __:__

Success: ‚òê Y  ‚òê N
SEQ (1-7): ___

Confusion Moments:
- 
- 

Detours/Backtracks (count): ___

Blockers Encountered:
- 
- 

Post-Task Confidence:
"Could you use this today?" ‚òê Y  ‚òê N
Why: ___________

Notes:
```

### **Research Output Rater Sheet**

```
Task: ‚òê T3 (Quick)  ‚òê T4 (Deep)
Company: ___________

Actionability (1-5): ___
Signal Coverage (1-5): ___
Decision-Maker Clarity (1-5): ___
Personalization Hooks (1-5): ___
Evidence/Sources (1-5): ___

Average: ___

Would you forward this to your team as-is?
‚òê Y  ‚òê N

Notes:
```

### **Accessibility Quick Pass**

```
Task: T___

Keyboard-Only:
‚òê Could complete
‚òê Any traps? (describe)
‚òê Focus order logical

Screen Reader:
‚òê Labels make sense
‚òê Announcements clear
‚òê Navigation intuitive

Visual:
‚òê Contrast sufficient
‚òê Target sizes adequate
‚òê Focus indicators visible

Notes:
```

---

## 6Ô∏è‚É£ **Testing Protocol**

### **Before Each Session**
1. Clear browser cache/cookies
2. Start with correct test account
3. Have sample CSVs ready
4. Screen recording enabled
5. Observer sheet printed

### **During Testing**
1. **Don't guide** - Let user explore
2. **Think-aloud** - Ask user to narrate
3. **Note exact wording** - Capture confusion quotes
4. **Mark detours** - Count backtracks
5. **60-second rule** - If stuck for 60s, offer one nudge: "What would you try next?"

### **After Each Task**
1. Ask SEQ: "How easy was this task?" (1-7)
2. Ask: "Could you use this today?" (Y/N + why)
3. Note any suggestions

### **After All Tasks**
1. SUS survey (10 questions)
2. Three open-ended questions:
   - "What felt fastest?"
   - "Where did you lose confidence?"
   - "What would you remove to make this simpler?"

---

## 7Ô∏è‚É£ **Evidence to Capture**

### **Screenshots/Videos**
- [ ] Onboarding steps (all 5)
- [ ] Dashboard states (empty, seeded, low credits)
- [ ] Research outputs (Quick and Deep)
- [ ] Bulk CSV status screens
- [ ] Low-credit warnings
- [ ] Save confirmation
- [ ] Settings changes

### **System Events** (if available)
- [ ] research_outputs creation
- [ ] usage_logs credit debits
- [ ] CSV job state transitions
- [ ] chat message persistence

### **Participant Quotes**
- Moments of confusion
- Moments of delight
- Suggestions for improvement
- Exact wording used

---

## 8Ô∏è‚É£ **Daily Debrief & Triage**

### **After Each Testing Day**
1. **Top 5 Issues** with clips
2. **Label**: S1/S2/S3 and Flow/Device/State
3. **Ask**: "If this 'passed' in lab, why could it fail in the field?"
4. **Assign owners** for fixes
5. **Re-test** T1/T2/T7 after fixes (riskiest seams)

### **Challenge Assumptions**
- "AE's will configure everything up front" ‚Üí Reality: just-in-time config
- "Users read instructions" ‚Üí Reality: they scan and click
- "Approval email is obvious" ‚Üí Reality: check spam, timing issues
- "CSV format is clear" ‚Üí Reality: various Excel exports

---

## 9Ô∏è‚É£ **Ground Truth (From Repo)**

### **What We're Testing Against**
1. ‚úÖ Onboarding is 5 steps, ‚â§ 3 minutes
2. ‚úÖ Quick Brief ‚â§ 2 minutes
3. ‚úÖ Deep Intelligence ‚â§ 10 minutes
4. ‚úÖ Agents deliver promised structure
5. ‚úÖ Bulk Research resilient to refresh
6. ‚úÖ Credits system visible & clear
7. ‚úÖ Signup/Approval flow smooth

---

## üîü **TL;DR Run-Sheet (Start Here)**

### **Day 1 Prep**
1. ‚òê Set up 2 test accounts (empty/seeded)
2. ‚òê Verify email approval working
3. ‚òê Load sample CSVs
4. ‚òê Print observer sheets
5. ‚òê Enable screen recording

### **Day 1 Testing**
1. ‚òê Run T1-T7 in order
2. ‚òê Record all interactions
3. ‚òê Collect SEQ per task
4. ‚òê Collect SUS at end

### **Day 1 Debrief**
1. ‚òê Surface top 5 problems with clips
2. ‚òê Assign owners
3. ‚òê Fix quickly

### **Day 2**
1. ‚òê Re-test T1/T2/T7 post-fix
2. ‚òê Repeat with next cohort
3. ‚òê Validate fixes didn't break anything

---

## üìä **Success Metrics Dashboard**

```
Task Completion Rates:
T1: ___% (target: ‚â•95%)
T2: ___% (target: ‚â•95%)
T3: ___% (target: ‚â•95%)
T4: ___% (target: ‚â•95%)
T5: ___% (target: ‚â•95%)
T6: ___% (target: ‚â•95%)
T7: ___% (target: ‚â•95%)
T8: ___% (target: ‚â•90%)
T9: ___% (target: ‚â•95%)
T10: ___% (target: ‚â•90%)

Average Times:
T2: ___ min (target: ‚â§3)
T3: ___ min (target: ‚â§2)
T4: ___ min (target: ‚â§10)
T7: ___ min (target: ‚â§5)

Usability:
SUS: ___ (target: ‚â•85)
SEQ (median): ___ (target: ‚â•6)

Quality:
Output Rating: ___ (target: ‚â•4)
Deep > Quick: ‚òê Y  ‚òê N

Accessibility:
AA Compliant: ‚òê Y  ‚òê N
Critical Issues: ___
```

---

## ‚úÖ **Ready to Test**

**This plan is**:
- ‚úÖ Grounded in first principles
- ‚úÖ Based on actual user flows
- ‚úÖ Measurable and actionable
- ‚úÖ Focused on outcomes, not pixels
- ‚úÖ Accessibility-aware
- ‚úÖ Ready for real participants

**Next Steps**:
1. Print observer sheets
2. Set up test accounts
3. Run through tasks yourself first
4. Recruit participants
5. Test, observe, iterate!
