# ğŸ” Comprehensive Prompt Strategy Audit & Optimization

**Date**: October 22, 2025  
**Scope**: Full evaluation against GPT-5 best practices and Responses API capabilities

---

## ğŸ“Š Executive Summary

### **Critical Finding**: Our prompts are not leveraging GPT-5's strengths

**Current Grade**: â­â­âš ï¸ (2.5/5)
- âœ… Good: User context, terminology adaptation
- âš ï¸ Weak: Instruction hierarchy, no tool preambles, contradictions
- âŒ Missing: Responses API features, persistence instructions, reasoning control

**Optimized Grade Target**: â­â­â­â­â­ (5/5)

---

## ğŸ¯ Application Context

### **Purpose**
B2B research intelligence platform for enterprise Account Executives to:
1. Research companies with ICP scoring
2. Identify buying signals and decision makers
3. Generate personalized outreach intelligence
4. Learn and adapt to user preferences over time

### **Target Users**
- Enterprise Account Executives (AEs)
- Sales Development Reps (SDRs)  
- Sales leaders needing strategic intelligence
- Expectations: Fast, accurate, actionable insights

### **Core Value Proposition**
Transform hours of manual research into seconds with AI-powered intelligence that:
- Discovers hidden ICP matches through deep analysis
- Generates hyper-personalized outreach angles
- Anticipates information needs before asking
- Learns user preferences automatically
- Delivers strategic insights beyond surface data

---

## ğŸ”´ Critical Issues in Current Prompt

### **1. Missing Instruction Hierarchy** âš ï¸
**Current**: Flat list of rules with no priority ordering  
**Issue**: GPT-5 wastes reasoning tokens trying to reconcile conflicts  
**Impact**: Slower, less predictable responses

**Example Contradiction**:
```
"Be concise but complete" â† Conflicting
"deep by default" vs "Ask clarifying questions" â† Contradictory
"Never ask what to research" vs "Ask ONE clarifying question when essential" â† Which takes priority?
```

**From Guide**: 
> "Poorly-constructed prompts containing contradictory or vague instructions can be more damaging to GPT-5 than to other models, as it expends reasoning tokens searching for a way to reconcile the contradictions"

### **2. No Tool Preambles** âŒ
**Current**: Silent tool calling - users see nothing until results  
**Issue**: Poor UX, especially on longer research tasks  
**Impact**: Users don't know what's happening

**From Guide**:
> "Intermittent model updates on what it's doing with its tool calls and why can provide for a much better interactive user experience - the longer the rollout, the bigger the difference these updates make."

**Should Have**:
- "ğŸ” Searching for recent funding rounds and leadership changes..."
- "ğŸ’¡ Found 3 key buying signals from last 30 days..."
- "âœ… Analysis complete. Synthesizing strategic recommendations..."

### **3. No Persistence Instructions** âŒ
**Current**: No guidance on when to stop or continue  
**Issue**: Agent may stop prematurely or ask unnecessary clarifications  
**Impact**: Incomplete research, interrupted flow

**From Guide**:
```xml
<persistence>
- You are an agent - keep going until the query is completely resolved
- Only terminate when you are sure the problem is solved
- Never stop when you encounter uncertainty â€” research or deduce
- Do not ask the human to confirm assumptions
</persistence>
```

### **4. "PREFERENCE CHECK-OUT" Anti-Pattern** ğŸš¨
**Current**: Lines 288-291 literally say "PREFERENCE CHECK-OUT"  
**Issue**: We JUST removed this language as "awkward/formal" in our UX fixes!  
**Impact**: Generates the exact language we're trying to eliminate

**This is in DIRECT CONFLICT with our recent UX improvements**

### **5. Not Using Responses API Features** âŒ
**Current**: Basic Chat Completions approach  
**Missing**:
- `previous_response_id` for efficient multi-turn
- `reasoning` parameter for effort control  
- `verbosity` parameter for output control
- `store: true` for conversation persistence
- Structured conversation state

**From API Docs**:
> "We've seen statistically significant improvements when using the Responses API... Tau-Bench Retail score increases from 73.9% to 78.2% just by switching"

### **6. No Context Gathering Strategy** âŒ
**Current**: "Make multiple web_search calls" (vague)  
**Issue**: No guidance on breadth vs depth, stopping criteria  
**Impact**: Over-searching or under-searching

**From Guide**:
```xml
<context_gathering>
Goal: Get enough context fast. Parallelize discovery and stop as soon as you can act.
Method:
- Start broad, then fan out to focused subqueries
- Launch varied queries in parallel; read top hits
- Avoid over-searching

Early stop criteria:
- You can name exact content to change
- Top hits converge (~70%) on one area
</context_gathering>
```

### **7. No Reasoning Effort Guidance** âŒ
**Current**: Not using `reasoning_effort` parameter  
**Issue**: All tasks use same reasoning level (inefficient)  
**Impact**: Wasted tokens on simple tasks, insufficient depth on complex ones

**Should Have**: Dynamic reasoning by task type
- Quick brief: `low` or `medium`
- Deep research: `high`
- Follow-ups: `low`

### **8. Prescriptive Format Requirements** âš ï¸
**Current**: Very specific section requirements ("Executive Summary MUST...")  
**Issue**: May conflict with GPT-5's natural structuring  
**Impact**: Forced, unnatural output

**From Guide (Cursor example)**:
> "Structured, scoped prompts yield the most reliable results... but avoid over-constraining format"

---

## âœ… Strengths to Preserve

### **1. User Context Injection** âœ…
**Good**: Profile, ICP, custom criteria, signals integrated  
**Keep**: This provides essential personalization

### **2. Terminology Adaptation** âœ…  
**Good**: Uses user's exact words ("Buying Signals" â†’ their term)  
**Keep**: Makes responses feel personalized

### **3. Learned Preferences** âœ…
**Good**: Behavioral learning from past interactions  
**Keep**: Core value proposition

### **4. Zero Clarifier Intent** âœ…
**Good**: Tries to avoid asking questions  
**Keep**: Aligns with proactive research goal  
**Fix**: Make it non-contradictory with other instructions

---

## ğŸ¯ Optimized Prompting Strategy

### **Architecture Changes**

#### **1. Adopt Instruction Hierarchy** (GPT-5 Best Practice)
```xml
<instruction_hierarchy>
Priority 1: User's explicit request (NEVER override this)
Priority 2: Complete research autonomously before yielding
Priority 3: Balance speed/depth based on learned preferences
Priority 4: Format preferences and output style
</instruction_hierarchy>
```

#### **2. Add Tool Preambles** (GPT-5 Best Practice)
```xml
<tool_preambles>
Before first tool use: "ğŸ” Researching [Company] â€” searching for [specific angles]..."
During research: Brief progress updates (5-10 words max)
After completion: "âœ… Analysis complete. Here's what I found..."

Keep preambles friendly and concise. Think trusted teammate, not robot.
</tool_preambles>
```

#### **3. Add Persistence** (GPT-5 Best Practice)
```xml
<persistence>
You are an autonomous research agent. Complete the task before yielding:
- Find at least 3 actionable insights before stopping
- If initial searches are surface-level, dig deeper automatically
- Never ask "should I continue?" â€” determine completeness yourself
- Only stop when you have specific recommendations
- Don't ask permission to research deeper â€” just do it
</persistence>
```

#### **4. Add Context Gathering Strategy** (GPT-5 Best Practice)
```xml
<context_gathering>
Strategy for maximum value:
1. Launch 3-5 parallel web_search calls immediately
   - Company + "funding news"
   - Company + "leadership changes"
   - Company + "tech stack hiring"
   - ICP-specific signals
   
2. Read top 2-3 results per search
3. Stop when you can answer with confidence (don't over-search)
4. Deep dive only if initial results are thin

Quality threshold: Every insight must be specific and unexpected
</context_gathering>
```

#### **5. Use Responses API Features**
```typescript
// In route.ts, update to:
const responseStream = await openai.responses.stream({
  model: 'gpt-5-mini',
  instructions, // Our optimized prompt
  input: lastUserMessage.content,
  previous_response_id: previousResponseId, // Multi-turn context
  reasoning: {
    effort: agentType === 'quick' ? 'low' : 'medium' // Dynamic
  },
  verbosity: 'medium', // Can override in prompt for specific sections
  store: true, // Enable conversation persistence
  tools: [{ type: 'web_search' }],
  metadata: {
    user_id: user.id,
    chat_id: chatId,
    agent_type: agentType
  }
});
```

#### **6. Remove Contradictions**
**REMOVE**:
- "Be concise but complete" â†’ Pick one based on user preference
- "Deep by default" + "Ask clarifying questions" â†’ Be autonomous
- "PREFERENCE CHECK-OUT" â†’ Remove entirely (we just fixed this!)

**ADD**:
- Clear priority hierarchy
- Autonomous operation by default
- Natural follow-up suggestions (not forced)

#### **7. Simplify Format Requirements**
**Current**: Prescriptive sections  
**New**: Outcome-focused guidance

```xml
<output_excellence>
Goal: Deliver decision-ready intelligence

Required elements:
1. Quick summary (2-3 sentences) with recommendation
2. Key insights (5-7 specific, unexpected findings)
3. Action items (who to contact, when, why)
4. Sources (3+ citations)

Structure flexibly around the story the data tells.
Adapt sections to fit the findings, not the template.
</output_excellence>
```

#### **8. Add Verbosity Control**
```xml
<verbosity_control>
Status updates: Ultra-brief (5-10 words)
Research findings: Detailed with evidence
Recommendations: Clear, numbered, actionable  
Summaries: Bullet points only

When user preference = "concise": Prioritize bullets over paragraphs
When user preference = "detailed": Include context and reasoning
</verbosity_control>
```

---

## ğŸ“ Rewritten Prompt (Full Implementation)

See `PROMPT_V2_OPTIMIZED.md` for complete implementation.

**Key Improvements**:
1. âœ… Clear instruction hierarchy
2. âœ… Tool preambles for UX
3. âœ… Persistence instructions
4. âœ… Context gathering strategy
5. âœ… No contradictions
6. âœ… Removed "PREFERENCE CHECK-OUT"
7. âœ… Flexible format guidance
8. âœ… Verbosity control
9. âœ… Reasoning effort by task type
10. âœ… Structured with XML tags

---

## ğŸ“Š Expected Impact

### **Performance Improvements**
- **Speed**: 15-25% faster (better context gathering, lower reasoning on simple tasks)
- **Quality**: Higher Tau-Bench scores (following Responses API best practices)
- **UX**: Much better (tool preambles, progress updates)
- **Consistency**: More predictable (clear hierarchy, no contradictions)

### **User Experience**
- **Before**: Silent â†’ sudden response (confusing)
- **After**: "ğŸ” Researching..." â†’ "ğŸ’¡ Found..." â†’ "âœ… Complete"

### **Cost Efficiency**
- Dynamic reasoning effort saves tokens on simple tasks
- `previous_response_id` reduces redundant context
- Better stop criteria prevents over-searching

---

## ğŸš€ Implementation Plan

### **Phase 1: Core Rewrite** (30 min)
1. âœ… Replace current prompt in `context.ts`
2. âœ… Add instruction hierarchy
3. âœ… Add tool preambles
4. âœ… Add persistence
5. âœ… Remove contradictions

### **Phase 2: API Updates** (20 min)
1. âœ… Add `previous_response_id` tracking
2. âœ… Add dynamic `reasoning_effort`
3. âœ… Add `verbosity` parameter
4. âœ… Enable `store: true`

### **Phase 3: Testing** (30 min)
1. Test quick brief (should be fast, low reasoning)
2. Test deep research (should be thorough, high reasoning)
3. Test follow-ups (should use previous_response_id)
4. Verify tool preambles show in UI

### **Phase 4: Iteration** (ongoing)
1. Monitor user feedback
2. Use metaprompting to refine
3. A/B test variations

---

## ğŸ“ References

**From GPT-5 Prompting Guide**:
- Instruction hierarchy (lines 270-293)
- Tool preambles (lines 77-125)
- Persistence (lines 67-73)
- Context gathering (lines 22-61)
- Cursor case study (lines 211-254)

**From Responses API Docs**:
- `previous_response_id` benefits (line 135)
- `reasoning` parameter (lines 109)
- `verbosity` parameter (lines 115)
- Multi-turn state (lines 96-106)

---

## âœ… Success Criteria

**We'll know it's working when**:
1. âœ… Users see progress updates during research
2. âœ… Responses come faster (dynamic reasoning)
3. âœ… No more contradictory behavior
4. âœ… Follow-ups are more natural
5. âœ… No "PREFERENCE CHECK-OUT" language
6. âœ… Consistent output quality
7. âœ… Better user satisfaction scores

---

**Status**: Audit complete, ready to implement  
**Confidence**: High (based on official best practices)  
**Risk**: Low (preserves what works, fixes what doesn't)
